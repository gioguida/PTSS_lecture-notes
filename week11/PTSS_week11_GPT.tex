\documentclass[11pt, a4paper]{article}

% --- PREAMBLE ---
% Set up packages for math, code, graphics, and layout
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[margin=1.5in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{palatino}
\usepackage{mathpazo}
\usepackage{microtype}

% Define colors for code listings
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.95}

% Configure the 'listings' package for C++
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2,
    language=C++,
    morecomment=[l]{//},
    morecomment=[s]{/*}{*/}
}
\lstset{style=mystyle}

% Allow line breaks in inline \texttt{} commands at underscores and other characters
\usepackage{xspace}

% Improve hyphenation and line breaking
\tolerance=2000
\emergencystretch=3em
\hfuzz=0.5pt

% Setup for the title page
\title{Programming Techniques for Scientific Simulations I:\\A Detailed Textbook}
\author{Based on lecture slides}
\date{\today}

% Hyperlink setup
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    pdftitle={Programming Techniques for Scientific Simulations},
    pdfpagemode=FullScreen,
}

% --- DOCUMENT START ---
\begin{document}

\maketitle
\tableofcontents
\newpage

\section{Introduction}

Scientific simulations often push modern hardware to its limits. Whether solving
partial differential equations, running Monte Carlo simulations, or performing
large-scale numerical optimization, performance is a central concern. While high-level
algorithmic improvements usually deliver the largest performance gains, low-level
implementation details---especially in languages like C++---can also be decisive.

C++ occupies a unique place in scientific computing:
\begin{itemize}
    \item It offers low-level control similar to C, enabling efficient memory access,
          data layout control, and predictable performance.
    \item It provides high-level abstractions such as classes, templates,
          operator overloading, and generic programming.
    \item Its compilation model enables advanced optimizations, some performed
          \emph{at compile time} via template instantiation.
\end{itemize}

This combination of abstraction and performance makes C++ a prime candidate for
scientific computing frameworks, but it also means that programmers must understand
how the compiler works. Well-designed C++ code allows the compiler to
generate extremely efficient machine code; poorly designed abstractions, on the
other hand, can introduce hidden loops, copies, and inefficiencies.

This chapter presents a detailed study of C++-specific optimization techniques,
starting with relatively familiar concepts such as \emph{inline functions} and
\emph{return value optimization}, and building up to advanced frameworks such as
\emph{template metaprogramming}, \emph{lazy evaluation}, and \emph{expression
templates}---the foundation of high-performance C++ linear algebra libraries like
Eigen, Blaze, and Blitz++.

Throughout this text, our goal is twofold:
\begin{enumerate}
    \item To explain the underlying principles, building intuition and
          clear mental models of how the compiler generates optimized code.
    \item To provide concrete examples, syntax templates, diagrams, and
          detailed discussions of scope, efficiency, and correctness, ensuring a
          complete reference suitable for both beginners and experienced C++
          programmers.
\end{enumerate}

We proceed now to the first major topic: C++-specific optimization features.

\section{Overview of C++ Optimization Techniques}

The slide deck begins by distinguishing between \emph{general} optimization
techniques—those that apply to nearly all programming languages—and
\emph{C++-specific} optimizations, which exploit the features and compilation model
unique to C++.

General optimizations include:
\begin{itemize}
    \item Reducing algorithmic complexity.
    \item Improving memory locality.
    \item Avoiding unnecessary computations.
    \item Using efficient data structures.
\end{itemize}

These apply regardless of programming language. In this chapter, however, we
focus exclusively on features provided by C++ that enable the compiler to generate
extremely efficient code, often eliminating entire loops or computations at compile
time.

According to the slides (page 2)\footnote{Slide reference: Week 11 lecture PDF, page~2.},
the C++-specific optimizations we will investigate include:
\begin{itemize}
    \item \textbf{Inlining}
    \item \textbf{Copy elision and Named Return Value Optimization (NRVO)}
    \item \textbf{Template metaprogramming}
    \item \textbf{Lazy evaluation}
    \item \textbf{Expression templates}
\end{itemize}

Each of these topics is more powerful than it may initially appear. For example:
\begin{itemize}
    \item Inlining does more than remove function call overhead—it exposes code
          to further optimizations.
    \item NRVO can eliminate copies even when copy constructors have side effects,
          and is guaranteed in many cases since C++17.
    \item Template metaprogramming effectively transforms the C++ compiler into a
          compile-time computation engine.
    \item Lazy evaluation avoids temporaries by deferring computation until required.
    \item Expression templates allow complex vector and matrix expressions to be
          collapsed into single optimal loops, rivaling Fortran performance.
\end{itemize}

The remainder of this chapter will explore these ideas in depth.

\section{Inlining in C++}

Inlining is one of the simplest and most widely used compiler optimizations in C++.
The basic idea is straightforward: instead of generating a function call, the
compiler substitutes the function's body directly at each call site. This can reduce
overhead and, more importantly, expose opportunities for further optimization.

\subsection{What is Inlining?}

A traditional function call requires several steps:
\begin{enumerate}
    \item pushing arguments on the stack,
    \item saving registers,
    \item jumping to the function body,
    \item executing instructions,
    \item returning to the caller.
\end{enumerate}

Although modern compilers optimize much of this overhead away, it still exists.
When inlining occurs, the function call is replaced with the function’s body itself,
eliminating the need for a call instruction.

\subsection{Syntax for Inlining}

The C++ keyword \texttt{inline} suggests to the compiler that a function may be inlined:

\begin{lstlisting}[caption={A simple inline function example}]
inline double square(double x) {
    return x * x;
}
\end{lstlisting}

\paragraph{Syntax template.}
\begin{verbatim}
inline return_type function_name(parameter_list) {
    statements...
}
\end{verbatim}

\paragraph{Component breakdown.}
\begin{itemize}
    \item \texttt{inline} is a hint, not a command. The compiler may ignore it.
    \item The function body must be visible at compile time to be inlined.
    \item Inline functions are often placed in headers.
\end{itemize}

\subsection{Why Inlining Improves Performance}

Inlining increases optimization opportunities:
\begin{itemize}
    \item It may allow constant propagation.
    \item Computations may be folded at compile time.
    \item Loops involving inline functions may be unrolled more effectively.
    \item The compiler may eliminate dead code revealed by inlining.
\end{itemize}

Inlining is also extremely important for template-based code, where most functions
are defined in header files and are therefore available to the compiler.

\subsection{Diagram Placeholder}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{placeholder_inline_expansion_diagram.png}
    \caption{Illustration of inline expansion: the function call is replaced by the body of the function.}
\end{figure}

\subsection{Practical Notes}
\begin{itemize}
    \item Excessive inlining can \emph{increase} code size and reduce instruction cache
          efficiency.
    \item Compilers today use sophisticated heuristics; explicitly marking a function
          \texttt{inline} primarily influences linkage and One Definition Rule (ODR) handling.
    \item For performance-critical code, especially in templated libraries (Eigen, Blaze,
          STL), inlining is essential and relied upon heavily.
\end{itemize}


\section{Copy Elision and Named Return Value Optimization (NRVO)}

Copy elision is one of the most important optimizations in C++. It eliminates
unnecessary temporary objects, often resulting in dramatic performance improvements.
Slides 4--5 of the lecture highlight how modern C++ compilers can construct objects
directly in their final location.

\subsection{What is Copy Elision?}

Copy elision refers to circumstances where the compiler is allowed to omit
copying or moving an object even if the copy/move constructor has observable side
effects. This is a unique exception to the usual C++ rule that side effects must be
preserved.

Copy elision typically removes:
\begin{itemize}
    \item copies of temporaries during return statements,
    \item copies created during initialization,
    \item intermediate temporaries in complex expressions.
\end{itemize}

\subsection{Return Value Optimization (RVO)}

Return Value Optimization is a specific form of copy elision that applies when a
function returns a temporary object.

\begin{lstlisting}[caption={Simple RVO example}]
C f() {
    return C();     // Temporary constructed directly into caller's space
}
\end{lstlisting}

Before C++17, RVO was an optimization the compiler was permitted to perform.
Since C++17, RVO is \emph{mandatory} when returning a prvalue.

\subsection{Named Return Value Optimization (NRVO)}

NRVO applies when a function returns a \emph{named} local variable:

\begin{lstlisting}[caption={Example of NRVO}]
C g() {
    C c;
    return c;       // NRVO: c is constructed directly in the caller
}
\end{lstlisting}

Whether NRVO is guaranteed depends on the exact conditions, but most compilers
perform it aggressively.

\subsection{Why Copy Elision Matters}

Avoiding copies:
\begin{itemize}
    \item reduces memory movement,
    \item improves cache locality,
    \item eliminates calls to (potentially expensive) copy constructors,
    \item enables zero-cost abstractions.
\end{itemize}

These are critical in scientific computing, where vectors, matrices, and other large
objects should not be copied unnecessarily.

\subsection{Copy Elision Example (from Slide 5)}

The lecture presents the following code:

\begin{lstlisting}[caption={Copy elision example illustrating RVO and NRVO}]
#include <iostream>

struct C {
    C() { std::cout << "C ctor\n"; }
    C(C const& c) { std::cout << "C copy ctor\n"; }
};

C f() {
    return C();     // RVO (C++17: mandatory)
}

C g() {
    C c;
    return c;       // NRVO (likely)
}

int main() {
    C c1 = f();     // no copies
    C c2 = g();     // usually no copies
}
\end{lstlisting}

If you compile this example with different flags:
\begin{itemize}
    \item \texttt{-std=c++03}
    \item \texttt{-std=c++11}
    \item \texttt{-std=c++17}
    \item \texttt{-fno-elide-constructors}
\end{itemize}
you will observe different behaviors in when copies are elided.

\subsection{Diagram Placeholder: Object Construction Paths}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{placeholder_copy_elision_flowchart.png}
    \caption{Flowchart showing how RVO and NRVO eliminate intermediate temporaries by constructing objects directly into the final storage location.}
\end{figure}

\subsection{Syntax Template for Functions Benefiting from RVO}

\begin{verbatim}
T function_name(parameters) {
    return T(constructor_args);  // RVO
}

T function_name(parameters) {
    T local(...);
    return local;                // NRVO
}
\end{verbatim}

\paragraph{Key point.}
Objects should be returned by value in modern C++. RVO/NRVO will avoid the copy,
and using return-by-value enables move semantics and copy elision.

\subsection{Practical Guidelines}

\begin{itemize}
    \item Do not prematurely optimize using \texttt{return std::move(obj)}; this may
          \emph{disable} copy elision.
    \item Prefer returning by value unless you explicitly need a reference.
    \item Know that modern compilers generate highly optimized return paths.
\end{itemize}

\section{Template Metaprogramming (TMP)}

Template Metaprogramming (TMP) is one of the most surprising and powerful features of C++.
Although templates were originally introduced to support generic programming, the C++ standard
allows templates to be instantiated recursively and to be specialized based on compile-time
constant values. As a consequence, C++ templates form a Turing-complete compile-time
programming language.

This section provides both the conceptual intuition and concrete examples needed to understand
TMP. We begin with the definition of ``meta'', explore the idea of the compiler as a computation
engine, and then walk through classical examples such as compile-time factorial and Erwin
Unruh’s famous prime-number program.

\subsection{What Does ``Meta'' Mean?}

The term ``meta'' refers to one level higher in abstraction. Slide 6 cites the
definition from the \emph{Free On-line Dictionary of Computing}: something is ``meta'' when it
describes or operates on something at a higher level.

Examples include:
\begin{itemize}
    \item \textbf{metasyntax}: syntax that defines other syntaxes,
    \item \textbf{metalanguage}: a language used to describe other languages,
    \item \textbf{metadata}: data about data,
    \item \textbf{metareasoning}: reasoning about reasoning.
\end{itemize}

Following this pattern, a \textbf{metaprogram} is:
\begin{quote}
    A program that writes, transforms, or manipulates other programs.
\end{quote}

In C++, a \emph{metaprogram} is a program that executes at compile time using templates. The
output of the metaprogram is C++ code itself, which is then executed at runtime.

\subsection{The C++ Compiler as a Turing Machine}

Slide 7 emphasizes a remarkable historical fact: Erwin Unruh demonstrated in 1994 that the C++
template system is Turing-complete. This means that:
\begin{itemize}
    \item Any computation expressible by a classical computer can be encoded using template
          instantiation.
    \item Compile-time recursion and branching are both possible.
    \item The halting problem applies: in general, it is undecidable whether the compiler will
          finish instantiating templates for arbitrary C++ code.
\end{itemize}

\subsubsection{Compile-Time Recursion}

Templates can refer to themselves with different template parameters:

\begin{lstlisting}[caption={Example pattern of compile-time recursion}]
template<int N>
struct Recursive {
    static constexpr int value = N + Recursive<N-1>::value;
};
\end{lstlisting}

\subsubsection{Compile-Time Branching via Specialization}

Template specialization lets you effectively write \emph{if-else} logic at compile time:

\begin{lstlisting}[caption={Template specialization as a compile-time branch}]
template<int N>
struct Compute {
    static constexpr int value = N * Compute<N-1>::value;
};

// Base case (like the "else" branch)
template<>
struct Compute<0> {
    static constexpr int value = 1;
};
\end{lstlisting}

\paragraph{Intuition analogy:}  
Think of templates as a set of \emph{rules}. The compiler repeatedly applies these rules,
instantiating templates with new arguments, much like a symbolic algebra system. When it reaches
a specialized template, recursion stops.

\subsection{Diagram Placeholder: Template Instantiation Tree}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.75\textwidth]{placeholder_template_instantiation_tree.png}
    \caption{A conceptual visualization of how templates instantiate recursively, forming a tree structure at compile time.}
\end{figure}

\subsection{Erwin Unruh’s Prime-Number Compiler Program}

Slide 8 presents one of the most historically significant TMP examples:  
a C++ program that causes the compiler to \emph{print all prime numbers} up to a limit---not at
run time, but as \texttt{error messages during compilation}.

This program was first shown to the ANSI/ISO C++ standards committee and was instrumental in
demonstrating that templates form a Turing-complete computation model.

Below is the simplified version (adapted to work with modern compilers):

\begin{lstlisting}[caption={Erwin Unruh's Compile-Time Prime Generator}]
template<int i> struct D { D(void*); operator int(); };

template<int p, int i>
struct is_prime {
    enum { prim = (p % i) && is_prime<(i > 2 ? p : 0), i-1>::prim };
};

template<int i>
struct Prime_print {
    Prime_print<i-1> a;
    enum { prim = is_prime<i, i-1>::prim };
    void f() { D<i> d = prim; }
};

// Base specializations to stop recursion
template<> struct is_prime<0,0> { enum { prim = 1 }; };
template<> struct is_prime<0,1> { enum { prim = 1 }; };

template<>
struct Prime_print<2> {
    enum { prim = 1 };
    void f() { D<2> d = prim; }
};

#ifndef LAST
#define LAST 10
#endif

void foo() {
    Prime_print<LAST> a;
}
\end{lstlisting}

\paragraph{How it works.}
\begin{itemize}
    \item The compiler recursively instantiates templates \texttt{Prime\_print<k>} for all
          \texttt{k} from \texttt{LAST} down to 2.
    \item For each number, it instantiates the \texttt{is\_prime<p,i>} template to check
          primality at compile time.
    \item Template instantiation triggers the creation of ill-formed code, forcing the compiler
          to emit an error diagnostic containing the prime number.
\end{itemize}

\paragraph{Takeaway:}  
Template metaprograms are full-blown \emph{algorithms}, executed entirely during
compilation.

\subsection{The “Hello World” of Template Metaprogramming: Compile-Time Factorial}

The slides (pages 9–10) show two versions of the factorial metaprogram. These are classical
examples because they illustrate:
\begin{itemize}
    \item compile-time recursion,
    \item specialization as a base case,
    \item use of \texttt{enum} or \texttt{static constexpr} for compile-time constants.
\end{itemize}

\subsubsection{Early C++ Version Using \texttt{enum}}

Before C++11, in-class static constant initialization was limited, so programmers often used
\texttt{enum} to store constant values.

\begin{lstlisting}[caption={Factorial computed at compile time (early C++ style)}]
// General case
template<int N>
struct Factorial {
    enum { value = N * Factorial<N-1>::value };
};

// Base case: stops recursion
template<>
struct Factorial<1> {
    enum { value = 1 };
};
\end{lstlisting}

\paragraph{Explanation of recursion:}
\[
F(N) = N \times F(N-1)
\quad\text{with}\quad F(1) = 1
\]

\paragraph{Example expansion (N=4):}
\begin{align*}
\text{Factorial<4>} &= 4 \times \text{Factorial<3>} \\
                    &= 4 \times (3 \times \text{Factorial<2>}) \\
                    &= 4 \times (3 \times (2 \times \text{Factorial<1>})) \\
                    &= 4 \times 3 \times 2 \times 1
\end{align*}

\subsubsection{Modern Version Using \texttt{static const} / \texttt{constexpr}}

Slide 10 shows the updated version:

\begin{lstlisting}[caption={Factorial TMP using static constant values (modern C++)}]
// General factorial template
template<int N>
struct Factorial {
    static const int value = N * Factorial<N-1>::value;
};

// Specialization for the base case
template<>
struct Factorial<1> {
    static const int value = 1;
};
\end{lstlisting}

\paragraph{Even better: C++11 and later.}

Using \texttt{constexpr}:

\begin{lstlisting}[caption={C++11+ constexpr factorial metaprogram}]
template<int N>
struct Factorial {
    static constexpr int value = N * Factorial<N-1>::value;
};

template<>
struct Factorial<0> {
    static constexpr int value = 1;
};
\end{lstlisting}

\subsection{Syntax Template for Compile-Time Recursion}

\begin{verbatim}
template<int N>
struct X {
    static constexpr int value = f(N, X<N-1>::value);
};

template<>
struct X<BASE> {
    static constexpr int value = BASE_RESULT;
};
\end{verbatim}

\paragraph{Scope and Semantics.}
\begin{itemize}
    \item Each instantiation \texttt{X<k>} is a distinct type.
    \item Values are computed at compile time.
    \item Recursion depth must be known at compile time.
\end{itemize}

\subsection{Diagram Placeholder: Compile-Time Factorial Expansion Tree}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{placeholder_factorial_compiletime_expansion.png}
    \caption{Visualization of recursive template instantiation for \texttt{Factorial<N>}.}
\end{figure}

\subsection{Key Takeaways}

\begin{itemize}
    \item C++ templates can perform arbitrary computations at compile time.
    \item Specializations act as compile-time branching.
    \item TMP is the foundation for advanced C++ techniques like:
          \begin{itemize}
              \item loop unrolling,
              \item static polymorphism,
              \item expression templates,
              \item high-performance numeric libraries.
          \end{itemize}
    \item Although TMP looks unusual, the underlying logic mirrors recursive functional
          programming.
\end{itemize}

\section{Loop Unrolling and Template-Based Optimization}

Many scientific computations repeatedly apply operations such as vector dot
products, small matrix multiplications, and short reductions. Although the
algorithmic complexity of such operations is trivial, their runtime performance is
surprisingly sensitive to low-level implementation details.

Slides 11--19 explore a key bottleneck:  
\textbf{short loops do not reach peak floating-point throughput}.  
This section explains why this happens and how C++ template metaprogramming provides a powerful solution: \emph{compile-time loop unrolling}.

\subsection{Performance Bottleneck in Dot Products}

Consider the classical dot product:

\begin{lstlisting}[caption={Standard dot product implementation},label=lst:dotbasic]
double dot(const double* a,
           const double* b,
           int N)
{
    double result = 0.;
    for (int i = 0; i < N; ++i) {
        result += a[i] * b[i];
    }
    return result;
}
\end{lstlisting}

As Todd Veldhuizen noted (Slide 11), this simple function often fails to reach the CPU’s theoretical peak performance on \textbf{small} vectors.

\subsubsection{Why Does This Happen?}

The loop introduces:
\begin{itemize}
    \item branch prediction overhead,
    \item loop counter increment and comparison,
    \item potential pipeline stalls,
    \item missed opportunities for vectorization.
\end{itemize}

When $N$ is, for example, 2, 3, or 4 (common for small vectors), the overhead dominates—the actual arithmetic operations take fewer CPU cycles than the loop mechanics.

\subsection{Manual Unrolling}

If the compiler knows the vector length at compile time, the loop can be manually unrolled:

\begin{lstlisting}[caption={Manually unrolled dot product for 3-element vectors}]
inline double dot3(const double* a, const double* b) {
    return a[0]*b[0]
         + a[1]*b[1]
         + a[2]*b[2];
}
\end{lstlisting}

This version eliminates:
\begin{itemize}
    \item loop control,
    \item branching,
    \item index arithmetic.
\end{itemize}

It is near optimal.

\subsection{Generalization: Using \texttt{TinyVector}}

Slides 12–19 introduce a fixed-size vector template:

\begin{lstlisting}[caption={Definition of a simple fixed-size vector class}]
template<typename T, int N>
class TinyVector {
public:
    T& operator[](int i)       { return data[i]; }
    T  operator[](int i) const { return data[i]; }
private:
    T data[N];
};
\end{lstlisting}

The question becomes:

\begin{quote}
    \emph{How can we automatically unroll the loop for any template parameter $N$?}
\end{quote}

Answer: \textbf{Template Metaprogramming}.

\subsection{Compile-Time Loop Unrolling via Template Metaprogramming}

Slide 13 presents the key structure: \texttt{meta\_dot}, a recursively defined template
that computes:

\[
\sum_{i=0}^{N-1} a[i] \cdot b[i]
\]

\begin{lstlisting}[caption={Metaprogram computing an unrolled dot product}]
template<int I>
struct meta_dot {
    template<typename T, int N>
    static T f(TinyVector<T,N>& a, TinyVector<T,N>& b) {
        return a[I] * b[I] + meta_dot<I-1>::f(a, b);
    }
};

// Termination of recursion
template<>
struct meta_dot<0> {
    template<typename T, int N>
    static T f(TinyVector<T,N>& a, TinyVector<T,N>& b) {
        return a[0] * b[0];
    }
};

// User-facing dot() function
template<typename T, int N>
inline T dot(TinyVector<T,N>& a, TinyVector<T,N>& b) {
    return meta_dot<N-1>::f(a, b);
}
\end{lstlisting}

\subsection{Understanding the Mechanism}

The compiler sees \texttt{meta\_dot<N-1>::f} and recursively expands:

\[
\texttt{meta\_dot<3>::f} \rightarrow
\texttt{meta\_dot<2>::f} \rightarrow
\texttt{meta\_dot<1>::f} \rightarrow
\texttt{meta\_dot<0>::f}
\]

This produces a fully unrolled expression at compile time.

\subsubsection{Compiler Expansion Walkthrough (Slides 14–19)}

The slides show the exact transformations the compiler performs.

Given:

\begin{lstlisting}
TinyVector<double,4> a, b;
double r = dot(a, b);
\end{lstlisting}

The expansion proceeds as follows:

\paragraph{Step 1}

\begin{verbatim}
dot(a, b) → meta_dot<3>::f(a, b)
\end{verbatim}

\paragraph{Step 2}

\begin{verbatim}
meta_dot<3>::f(a,b)
 → a[3]*b[3] + meta_dot<2>::f(a,b)
\end{verbatim}

\paragraph{Step 3}

\begin{verbatim}
meta_dot<2>::f(a,b)
 → a[2]*b[2] + meta_dot<1>::f(a,b)
\end{verbatim}

\paragraph{Step 4}

\begin{verbatim}
meta_dot<1>::f(a,b)
 → a[1]*b[1] + meta_dot<0>::f(a,b)
\end{verbatim}

\paragraph{Step 5 (Termination)}

\begin{verbatim}
meta_dot<0>::f(a,b)
 → a[0]*b[0]
\end{verbatim}

\paragraph{Final Expression}

\[
a[3]b[3] + a[2]b[2] + a[1]b[1] + a[0]b[0]
\]

No loop remains.  
No index arithmetic.  
No branching.  
Just raw arithmetic.

\subsection{Diagram Placeholder: Unrolling via Recursive Templates}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.82\textwidth]{placeholder_compile_time_unrolling_diagram.png}
    \caption{Illustration of how the template \texttt{meta\_dot} recursively expands into a fully unrolled sum at compile time.}
\end{figure}

\subsection{Key Observations}

\begin{itemize}
    \item The unrolled code is as efficient as manually written code.
    \item All recursion happens at compile time, so runtime has no overhead.
    \item Inlining is crucial: each recursive call must be inlined to avoid function-call overhead.
    \item The technique generalizes to many other operations (e.g., reductions, small matrix multiplications).
\end{itemize}

\subsection{Syntax Template: Compile-Time Loop Unrolling}

\begin{verbatim}
template<int I>
struct LoopUnroll {
    template<typename Vec>
    static auto compute(Vec& v) {
        return f(I, v) + LoopUnroll<I-1>::compute(v);
    }
};

template<>
struct LoopUnroll<0> {
    template<typename Vec>
    static auto compute(Vec& v) { return f(0, v); }
};
\end{verbatim}

\paragraph{Interpretation.}  
This is analogous to a \texttt{for}-loop, but the “loop counter” is encoded in the type
system, not in runtime variables.

\subsection{Why This Matters for Scientific Computing}

Scientific computations frequently involve:
\begin{itemize}
    \item small fixed-size vectors,
    \item coordinate transformations,
    \item SIMD-sized blocks,
    \item high-frequency arithmetic kernels.
\end{itemize}

Manually optimizing all these kernels is tedious and unmaintainable.  
\textbf{Template metaprogramming provides automatic compile-time optimization.}

Libraries such as Eigen, Blaze, MTL4, and Blitz++ rely on such techniques to achieve near–Fortran 77 performance while maintaining expressive high-level notation.

\section{Operator Overloading and Performance Pitfalls}

Operator overloading is one of C++’s most attractive features, especially for
scientific computing. It allows the programmer to write mathematical expressions
in a natural and intuitive way:

\[
a = b + c + d
\]

However, as shown in Slides 20–23, naïve operator overloading can result in severe
performance degradation. The root cause is that each binary operator (like
\texttt{operator+}) traditionally creates a temporary vector, leading to unnecessary
memory allocations and extra passes through arrays.

In this section we examine the problem in depth and motivate the need for
\textbf{lazy evaluation}, paving the way toward expression templates.

\subsection{A Simple Vector Class}

Slide 20 provides an example of a minimal vector class, which we reproduce here in a
cleaned-up form:

\begin{lstlisting}[caption={Naïve simple vector class},label=lst:simplevector]
template <typename T>
class simplevector {
public:
    typedef T value_type;
    typedef T& reference;
    typedef unsigned int size_type;

    // Constructor
    explicit simplevector(size_type s = 0);

    // Copy constructor
    simplevector(const simplevector& v);

    // Destructor
    ~simplevector();

    // Swap
    void swap(simplevector& v) {
        std::swap(p_, v.p_);
        std::swap(sz_, v.sz_);
    }

    // Copy assignment
    simplevector& operator=(simplevector v);

    // Compound addition
    simplevector& operator+=(const simplevector& v);

    // Size
    size_type size() const { return sz_; }

    // Subscript
    value_type operator[](size_type i) const;
    reference operator[](size_type i);

private:
    value_type* p_;
    size_type sz_;
};
\end{lstlisting}

A free function implements \texttt{operator+}:

\begin{lstlisting}[caption={Binary operator+ for simplevector}]
template <typename T>
simplevector<T> operator+(const simplevector<T>& x,
                          const simplevector<T>& y)
{
    simplevector<T> result = x; // Copies x
    result += y;                // Loop over all elements
    return result;              // Returns a temporary
}
\end{lstlisting}

\subsection{What Goes Wrong With Naïve Operator Overloading?}

Slides 20–21 explain the key issue.  
Consider the expression:

\[
a = b + c + d
\]

The compiler interprets it as:

\begin{verbatim}
a = (b + c) + d;
\end{verbatim}

Which translates into the following sequence of operations:

\subsubsection*{Step-by-step evaluation}

1. \textbf{Compute \texttt{b + c}}  
   \begin{itemize}
       \item invokes \texttt{operator+(b,c)}
       \item creates a temporary vector \texttt{tmp1}
       \item copies \texttt{b} into \texttt{tmp1}
       \item loops over \texttt{tmp1} to add \texttt{c}
   \end{itemize}

2. \textbf{Compute \texttt{tmp1 + d}}  
   \begin{itemize}
       \item creates \texttt{tmp2}
       \item loops over all elements
   \end{itemize}

3. \textbf{Assign to \texttt{a}}  
   \begin{itemize}
       \item loops again over all elements to copy \texttt{tmp2} into \texttt{a}
   \end{itemize}

\subsection{Total Operations}

For a vector of size $N$, the naïve approach performs:
\begin{itemize}
    \item 3 full-array loops,  
    \item 2 full-array temporary copies,  
    \item total of 5 full-array reads and 3 full-array writes.
\end{itemize}

This is dramatically worse than the ideal:

\[
\text{one loop:} \quad
a[i] = b[i] + c[i] + d[i]
\]

which requires:
\begin{itemize}
    \item 3 reads,
    \item 1 write,
    \item zero temporaries.
\end{itemize}

\subsection{Diagram Placeholder: Temporary Explosion in Naïve Operator+}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{placeholder_operator_overloading_temporaries.png}
    \caption{Evaluation of the expression $a = b + c + d$ under naïve operator
             overloading: each binary operator creates a temporary vector and loops
             over all elements.}
\end{figure}

\subsection{Benchmarking Notes}

Slide 21 suggests running tests such as:

\begin{verbatim}
$ g++ -O3 -march=native -fopt-info-loop ...
$ time -p ./a.out
\end{verbatim}

Comparing:
\begin{itemize}
    \item \texttt{timesimple.cpp} (naïve implementation),
    \item \texttt{timesimple\_handopt.cpp} (manual single-loop implementation),
    \item \texttt{timesimple\_handopt\_avoid.cpp} (optimally avoiding temporaries)
\end{itemize}

The manually optimized version dramatically outperforms the naïve overloaded version.

\subsection{Why C++ Needs Lazy Evaluation}

The core problem is:
\begin{quote}
    \textbf{Operator overloading gives beautiful syntax but disastrous performance.}
\end{quote}

We want:

\begin{verbatim}
a = b + c + d;
\end{verbatim}

to behave like:

\begin{lstlisting}
for (int i = 0; i < a.size(); ++i) {
    a[i] = b[i] + c[i] + d[i];
}
\end{lstlisting}

This requires the compiler to:
\begin{itemize}
    \item avoid creating temporaries,
    \item avoid extra loops,
    \item defer evaluation until assignment,
    \item build a representation of the \emph{entire expression} before executing it.
\end{itemize}

This is the motivation behind:
\begin{itemize}
    \item lazy evaluation,
    \item expression objects,
    \item expression templates (generalized lazy evaluation).
\end{itemize}

\subsection{Understanding How Naïve Addition Works Internally}

Slides 22–23 break down the default semantics:

Given:

\begin{lstlisting}
simplevector<double> a(N), b(N), c(N);
a = b + c;
\end{lstlisting}

The sequence is:

1. Construct temporary \texttt{tmp = b + c}  
2. \texttt{a = tmp} loops over the data  

The implementation of \texttt{operator+}:

\begin{lstlisting}
template <typename T>
simplevector<T> operator+(const simplevector<T>& x,
                          const simplevector<T>& y)
{
    simplevector<T> result = x; // copy
    result += y;               // loop
    return result;             // temporary
}
\end{lstlisting}

Copy constructor:

\begin{lstlisting}
simplevector(const simplevector& v)
    : p_(new value_type[v.size()]),
      sz_(v.size())
{
    for (size_type i = 0; i < size(); ++i)
        p_[i] = v.p_[i];
}
\end{lstlisting}

Assignment operator:

\begin{lstlisting}
simplevector& operator=(simplevector v) {
    swap(v);
    return *this;
}
\end{lstlisting}

Compound \texttt{operator+=}:

\begin{lstlisting}
simplevector& operator+=(const simplevector& v) {
    for (size_type i = 0; i < size(); ++i)
        p_[i] += v.p_[i];
    return *this;
}
\end{lstlisting}

\subsection{Diagram Placeholder: Memory Traffic Comparison}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.85\textwidth]{placeholder_memory_traffic_comparison.png}
    \caption{Comparison of memory traffic between naïve operator overloading
             (multiple temporaries, multiple loops) and the ideal implementation
             (single loop, zero temporaries).}
\end{figure}

\subsection{Key Takeaways}

\begin{itemize}
    \item Operator overloading is not inherently efficient.
    \item Without lazy evaluation or expression templates, each operator invocation
          triggers a full pass over the data.
    \item Performance can degrade by an order of magnitude or more.
    \item This issue motivated the development of \textbf{lazyvector} and more
          generally \textbf{expression templates}, covered in the next sections.
\end{itemize}

\section{Lazy Evaluation}

The previous section showed that naïve operator overloading eagerly evaluates every
subexpression and creates unnecessary temporaries.  
To achieve performance comparable to the manually optimized version, we must change
the evaluation strategy.

Slides 24–26 introduce the concept of \textbf{lazy evaluation}, where arithmetic
expressions are represented as objects instead of being computed immediately. The
final computation occurs only when assigning to a concrete vector.

This idea is fundamental to modern high-performance C++ libraries.

\subsection{What Is Lazy Evaluation?}

In standard C++ operator overloading, the expression:

\begin{lstlisting}
a = b + c;
\end{lstlisting}

is evaluated as:

\begin{enumerate}
    \item compute temporary \texttt{tmp = b + c};
    \item assign \texttt{tmp} to \texttt{a}.
\end{enumerate}

Lazy evaluation changes the model:

\begin{enumerate}
    \item \textbf{operator+ builds a representation of the expression} (an object).
    \item No data is computed immediately.
    \item \textbf{The assignment operator evaluates the entire expression in one loop}.
\end{enumerate}

Thus:

\begin{lstlisting}
a = b + c;
\end{lstlisting}

produces the optimal equivalent:

\begin{lstlisting}
for (int i = 0; i < a.size(); ++i)
    a[i] = b[i] + c[i];
\end{lstlisting}

with no temporaries, no extra loops, and no wasted memory.

\subsection{The \texttt{vectorsum} Class}

Slide 24 shows how to implement a class representing the expression $x + y$.

\begin{lstlisting}[caption={The vectorsum expression class}]
template <typename T>
class vectorsum {
public:
    typedef T value_type;
    typedef unsigned int size_type;

    vectorsum(const lazyvector<T>& x,
              const lazyvector<T>& y)
        : left_(x), right_(y) {}

    // Deferred element access:
    value_type operator[](size_type i) const {
        return left_[i] + right_[i];
    }

private:
    const lazyvector<T>& left_;
    const lazyvector<T>& right_;
};
\end{lstlisting}

\paragraph{Important.}
The \textbf{vectorsum} itself does not store computed values.  
Instead, it stores references to the operands and defines how to compute each element on demand.

\subsection{\texttt{operator+} Returns a \texttt{vectorsum}}

Instead of returning a fully computed vector, \texttt{operator+} returns a
\textbf{vectorsum expression object}:

\begin{lstlisting}[caption={Lazy operator+ implementation}]
template <typename T>
inline vectorsum<T>
operator+(const lazyvector<T>& x,
          const lazyvector<T>& y)
{
    return vectorsum<T>(x, y);
}
\end{lstlisting}

No computation happens at this stage.

\subsection{How Assignment Evaluates the Expression}

The \texttt{lazyvector} class implements:

\begin{lstlisting}[caption={Assignment operator with lazy evaluation}]
template <typename T>
class lazyvector {
public:
    // assignment from a vectorsum object
    lazyvector& operator=(const vectorsum<T>& v) {
        for (int i = 0; i < size(); ++i)
            p_[i] = v[i];   // computes element on demand
        return *this;
    }
};
\end{lstlisting}

This performs a single loop, evaluating:

\[
a[i] = b[i] + c[i]
\]

and eliminating temporaries completely.

\subsection{Putting It All Together}

Given:

\begin{lstlisting}
lazyvector<double> a(N), b(N), c(N);
a = b + c;
\end{lstlisting}

The process is:

\begin{enumerate}
    \item \texttt{b + c} constructs a \texttt{vectorsum} object referring to \texttt{b} and \texttt{c}.
    \item Assignment \texttt{a = vectorsum} triggers a loop:
          \[
              a[i] = b[i] + c[i]
          \]
\end{enumerate}

\subsection{Diagram Placeholder: Lazy Evaluation Workflow}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.85\textwidth]{placeholder_lazy_evaluation_workflow.png}
    \caption{Lazy evaluation avoids temporaries by turning expressions such as
             $b + c$ into symbolic objects and evaluating them only on assignment.}
\end{figure}

\subsection{Comparison With Naïve Evaluation}

Slide 26 compares three approaches:

\begin{enumerate}
    \item \textbf{Simple implementation}: multiple loops + temporaries → slow  
    \item \textbf{Hand-optimized}: one loop → fast  
    \item \textbf{Lazy evaluation}: one loop, same as hand-optimized → fast  
\end{enumerate}

Commands:

\begin{verbatim}
$ make timesimpl2 timelazy2 timesimple2_handopt
$ time -p ./timesimple2
$ time -p ./timesimple2_handopt
$ time -p ./timelazy2
\end{verbatim}

Lazy evaluation and hand-optimized code have comparable performance.

\subsection{Scope and Semantics}

\begin{itemize}
    \item \texttt{vectorsum<T>} has no data of its own except references to operands.
    \item The expression remains valid only as long as the operands remain alive.
    \item Assignment is the only time when data is actually written.
    \item Lazy evaluation works only for binary operations (so far).
\end{itemize}

\subsection{Limitations of Basic Lazy Evaluation}

The approach described so far supports only binary expressions like:

\[
a = b + c, \qquad a = b - c
\]

but cannot yet handle:

\[
a = b + c + d
\quad\text{or}\quad
a = b * (c + d) + \exp(e)
\]

because the expression types are not composable:
\begin{itemize}
    \item \texttt{vectorsum + lazyvector} is not defined,
    \item \texttt{vectorsum + vectorsum} is not defined,
    \item no generic mechanism exists to combine expressions recursively.
\end{itemize}

This sets the stage for \textbf{Expression Templates}, discussed starting in the next section.

\subsection{Key Takeaways}

\begin{itemize}
    \item Naïve operator overloading creates temporaries and loops.
    \item Lazy evaluation delays computation until assignment.
    \item Expressions become lightweight symbolic objects.
    \item Performance matches hand-optimized code.
    \item Next step: generalizing lazy evaluation to arbitrary expression trees via
          expression templates.
\end{itemize}

\section{Expression Templates}

Lazy evaluation solved the performance problem for binary expressions (such as
$a = b + c$), but it cannot yet handle more complex expressions like
$a = b + c + d$ or $a = b * (c + d)$.
The reason: the binary \texttt{vectorsum} type works only for a single
operation involving two operands. There is no general way to represent expression
trees of arbitrary shape.

Slides 27--37 introduce \emph{Expression Templates (ET)}, a powerful design pattern
that generalizes lazy evaluation to entire expression trees, enabling notation such as:

\[
a = b + c + d
\qquad\text{or}\qquad
a = b * (c + d) + \exp(e)
\]

with performance \textbf{identical} to hand-optimized code.

Expression Templates were independently developed by Todd Veldhuizen and David Vandevoorde in the mid-1990s, and they have since become a foundational technique for C++ numeric libraries.

\subsection{From Binary Expression Classes to General Expression Trees}

The class \texttt{vectorsum} only represents $x + y$.  
We want a general mechanism to represent:

\begin{itemize}
    \item any binary operator,
    \item on any pair of operands,
    \item where operands may themselves be expressions.
\end{itemize}

Slide 27 introduces the solution: \textbf{operation classes}.

\subsection{Operation Classes}

An operation class encapsulates a binary operator as a callable function:

\begin{lstlisting}[caption={Operation classes}]
struct plus {
    static inline double apply(double a, double b) {
        return a + b;
    }
};

struct minus {
    static inline double apply(double a, double b) {
        return a - b;
    }
};
\end{lstlisting}

This separates:
\begin{itemize}
    \item expression structure (trees), from
    \item the operation executed at each node.
\end{itemize}

\subsection{Generalizing the Expression Node: \texttt{vectorop}}

Slide 27 defines:

\begin{lstlisting}[caption={Templated vector operation class}]
template <typename T, typename Op>
class vectorop {
public:
    typedef unsigned int size_type;
    typedef T value_type;

    vectorop(const lazyvector<T>& x,
             const lazyvector<T>& y)
        : left_(x), right_(y) {}

    value_type operator[](size_type i) const {
        return Op::apply(left_[i], right_[i]);
    }

private:
    const lazyvector<T>& left_;
    const lazyvector<T>& right_;
};
\end{lstlisting}

\paragraph{Limitations:}  
Still only works for:
\[
\texttt{lazyvector} \,\texttt{Op}\, \texttt{lazyvector}
\]

We want:
\[
\texttt{expression} \,\texttt{Op}\, \texttt{expression}
\]

\subsection{Step 1: Generalize to a Universal Expression Template Class}

Slide 30 introduces the key abstraction:

\begin{lstlisting}[caption={General expression node template}]
template<typename Left, typename Right, typename Op>
class X {
public:
    X(const Left& x, const Right& y)
        : left_(x), right_(y) {}

    double operator[](int i) const {
        return Op::apply(left_[i], right_[i]);
    }

private:
    const Left&  left_;
    const Right& right_;
};
\end{lstlisting}

This generalizes everything:

\begin{itemize}
    \item \texttt{Left} and \texttt{Right} may both be:
        \begin{itemize}
            \item concrete vectors,
            \item expression nodes,
            \item scalar wrappers, etc.
        \end{itemize}
    \item \texttt{Op} determines how to combine the children.
    \item Recursive composition allows unlimited expression depth.
\end{itemize}

\paragraph{Intuition:}  
Each operator returns a node in a tree, not a computed vector.  
Assignment later walks the tree.

\subsection{Step 2: Generalizing Operator Overloading}

Previously, we had:

\begin{lstlisting}
vectorop<T,plus> operator+(lazyvector<T>, lazyvector<T>);
\end{lstlisting}

Now, we want to allow any combination of expressions.

Slide 31 defines a general \texttt{operator+}:

\begin{lstlisting}[caption={Generalized operator+ for expression templates}]
template<typename Left, typename T>
inline X<Left, etvector<T>, plus>
operator+(const Left& a, const etvector<T>& b)
{
    return X<Left, etvector<T>, plus>(a, b);
}
\end{lstlisting}

In a complete implementation, we need many overloads:
\begin{itemize}
    \item \texttt{expression + vector}
    \item \texttt{vector + expression}
    \item \texttt{expression + expression}
\end{itemize}

All create new \texttt{X<..., ..., plus>} objects.

\subsection{Step 3: Generalized Assignment}

Slides 32--37 show how assignment evaluates the expression:

\begin{lstlisting}[caption={Assignment from an expression}]
// inside etvector<T>
template <typename L, typename R, typename Op>
const etvector& operator=(const X<L, R, Op>& v) {
    for (int i = 0; i < size(); ++i)
        p_[i] = v[i];
    return *this;
}
\end{lstlisting}

\subsection{How Expression Templates Work Internally}

Given:

\begin{lstlisting}
D = A + B + C;
\end{lstlisting}

The compiler parses it left-to-right (Slides 33–37):

\paragraph{Step 1: Build subexpression}

\begin{lstlisting}
X<etvector<T>, etvector<T>, plus>(A, B)
\end{lstlisting}

\paragraph{Step 2: Combine with C}

\begin{lstlisting}
X< X<etvector<T>, etvector<T>, plus>,
   etvector<T>,
   plus >( X(A,B), C )
\end{lstlisting}

This tree is constructed entirely at compile time.

\paragraph{Step 3: Assignment triggers evaluation}

\begin{lstlisting}
D.operator=(the_expression)
\end{lstlisting}

\paragraph{Step 4: The loop}

\begin{lstlisting}
for (int i = 0; i < sz_; ++i) {
    p_[i] = the_expression[i];
}
\end{lstlisting}

\paragraph{Step 5: Element evaluation}

\begin{lstlisting}
the_expression[i]
 → plus::apply( X(A,B)[i], C[i] )
 → plus::apply( A[i] + B[i], C[i] )
 → A[i] + B[i] + C[i]
\end{lstlisting}

\subsection{Diagram Placeholder: Expression Tree Construction}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{placeholder_expression_tree_diagram.png}
    \caption{Construction of the expression tree for $D = A + B + C$ using
             expression templates. Each operator builds a node linking its operands.}
\end{figure}

\subsection{Diagram Placeholder: Assignment-Time Evaluation}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{placeholder_et_assignment_evaluation.png}
    \caption{During assignment, the result vector is computed in a single loop by
             recursively evaluating the expression tree.}
\end{figure}

\subsection{Syntax Template for Expression Template Classes}

\begin{verbatim}
template<typename L, typename R, typename Op>
class ExprNode {
public:
    ExprNode(const L& l, const R& r);

    value_type operator[](int i) const {
        return Op::apply(l[i], r[i]);
    }

private:
    const L& l;
    const R& r;
};

template<typename L, typename R, typename Op>
ExprNode<L,R,Op> operator+(const L&, const R&);
\end{verbatim}

\subsection{Key Features of Expression Templates}

Expression templates:

\begin{itemize}
    \item build expression trees at compile time,
    \item eliminate all intermediate temporaries,
    \item evaluate the entire expression in one loop,
    \item allow intuitive high-level syntax,
    \item achieve performance comparable to hand-written loops,
    \item maintain zero-overhead abstraction.
\end{itemize}

\subsection{Why Expression Templates Are So Powerful}

Expression templates enable:
\begin{itemize}
    \item “vectorized” evaluation without writing vectorized code,
    \item fusion of loops (loop fusion),
    \item compile-time optimization of expression structure,
    \item reuse of expression nodes,
    \item strong inlining opportunities.
\end{itemize}

This technique is used extensively in high-performance C++ frameworks such as:
\begin{itemize}
    \item Eigen
    \item Blaze
    \item MTL4
    \item Blitz++
    \item Armadillo++
    \item Boost uBLAS
\end{itemize}

\subsection{Expression Templates and Compile-Time Reasoning}

Note that expression templates:
\begin{itemize}
    \item do not need “template metaprogramming” in the classical recursive sense,
    \item but rely on compile-time polymorphism and type construction,
    \item turning the compiler into an expression optimizer.
\end{itemize}

They produce extremely efficient binary code, frequently matching or beating Fortran 90.

\subsection{Key Takeaways}

\begin{itemize}
    \item Lazy evaluation is generalized by expression templates.
    \item ET allows recursively nested expressions of arbitrary complexity.
    \item Assignment walks the expression tree in a single loop.
    \item No temporaries are created.
    \item Performance is optimal and abstraction costs are zero.
\end{itemize}

\section{Expression Templates in Practice}

Expression Templates (ET) began as a research idea in the mid-1990s. They quickly
proved powerful and have since become the backbone of modern C++ numerical and
scientific computing frameworks. Slides 38--41 illustrate this evolution with
historical context, example libraries, and performance data.

\subsection{Blitz++: The First Major Expression Template Library}

Slide 38 highlights \textbf{Blitz++}, created by Todd Veldhuizen.  
Blitz++ was the first C++ library to apply ET systematically and at scale.  

Blitz++ provides:
\begin{itemize}
    \item multi-dimensional arrays (\texttt{Array<T, D>}),
    \item fixed-size vectors (\texttt{TinyVector<T, N>}),
    \item small matrices,
    \item mathematical operations implemented using expression templates.
\end{itemize}

Its goal was to replace Fortran as the preferred HPC language while retaining C++’s
expressiveness. Although later libraries (e.g.\ Eigen) became more widely adopted,
Blitz++ remains an important milestone.

\subsection{Performance Results}

Slide 39 shows benchmark data comparing:

\begin{itemize}
    \item C with loops,
    \item Fortran 77,
    \item Fortran 90/95,
    \item C++ with ET.
\end{itemize}

Key insight:

\begin{quote}
    \textbf{C++ with expression templates achieves $>90\%$ of Fortran 77’s peak performance,  
    and often surpasses Fortran 90/95 or plain C.}
\end{quote}

This result is striking because Fortran has long been considered the gold standard
for numerically intensive HPC applications.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.85\textwidth]{placeholder_et_benchmark_results.png}
    \caption{Benchmark diagram comparing ET-enhanced C++ with Fortran and C.  
             (A placeholder representing Slide~39’s benchmark graph.)}
\end{figure}

\subsection{Modern HPC Frameworks Using Expression Templates}

Slide 40 lists several widely used C++ libraries that rely on ET internally:

\paragraph{Eigen}  
\begin{itemize}
    \item Highly optimized linear algebra library.
    \item Extensive use of expression templates to fuse operations and avoid temporaries.
    \item Supports high-level notation for matrices, vectors, factorizations, and solvers.
\end{itemize}

\paragraph{Blaze}  
\begin{itemize}
    \item High-performance library using smart ET mechanisms and SIMD vectorization.
    \item Focused on large-scale numerical simulation.
\end{itemize}

\paragraph{Armadillo++}  
\begin{itemize}
    \item User-friendly syntax, similar to MATLAB.
    \item Uses delayed evaluation and expression templates internally.
\end{itemize}

\paragraph{MTL4 (Matrix Template Library)}  
\begin{itemize}
    \item Emphasizes generic programming and mathematical notation.
\end{itemize}

\paragraph{Boost uBLAS}  
\begin{itemize}
    \item Expression-template-based linear algebra library within Boost.
    \item Wide ecosystem support and integration with other Boost libraries.
\end{itemize}

\paragraph{And many others \dots}

The prevalence of ET demonstrates that this technique is now a “standard” C++
idiom for high-performance numerics.

\subsection{Algorithmic-Derivation Skeleton Example}

The final slide (Slide~41) shows a generic expression template skeleton for symbolic
algebraic manipulation:

\begin{lstlisting}[caption={Algorithmic derivation skeleton using expression templates}]
template <typename T> class Constant;
template <typename T> class Variable;

enum OP_enum { Add, Multiply };

template<typename L, typename R, OP_enum op>
class Expression { };

template<typename L, typename R>
Expression<L, R, Multiply>
operator*(const L& l, const R& r) {
    return Expression<L, R, Multiply>(l, r);
}

template<typename L, typename R>
Expression<L, R, Add>
operator+(const L& l, const R& r) {
    return Expression<L, R, Add>(l, r);
}
\end{lstlisting}

This skeleton resembles the earlier \texttt{X<Left,Right,Op>} structure but uses an
enumeration to denote operations.  
Such a system can be used for automatic differentiation, symbolic algebra, or code
generation.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{placeholder_expression_tree_symbolic.png}
    \caption{A schematic (placeholder) visualization of symbolic expressions built  
             using expression templates.}
\end{figure}

\subsection{Concluding Remarks}

In this chapter, we explored a series of increasingly sophisticated C++
optimization techniques:

\begin{enumerate}
    \item \textbf{Inlining} exposes opportunities for deep optimization.
    \item \textbf{Copy elision (RVO/NRVO)} eliminates unnecessary temporaries.
    \item \textbf{Template metaprogramming} enables compile-time computation.
    \item \textbf{Compile-time loop unrolling} produces optimal arithmetic kernels.
    \item \textbf{Naïve operator overloading} is expressive but inefficient.
    \item \textbf{Lazy evaluation} defers computation and avoids temporaries.
    \item \textbf{Expression templates} generalize lazy evaluation to entire expression trees,
          enabling high-level notation with zero abstraction cost.
\end{enumerate}

These techniques underpin many modern C++ scientific computing libraries. Expression
templates, in particular, demonstrate how C++ can simultaneously provide elegant,
high-level abstractions and low-level, optimal performance.

\medskip

C++ remains a uniquely powerful language precisely because of this ability:  
to abstract without sacrificing speed.

\medskip

\noindent\textbf{End of Chapter.}

\newpage

\end{document}
